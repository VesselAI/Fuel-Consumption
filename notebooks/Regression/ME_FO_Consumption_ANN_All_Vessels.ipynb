{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e79209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "# settings\n",
    "MINIO_HOST = os.environ.get('MINIO_HOST', '')\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY', '')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY', '')\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"\"\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2fc723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes\n",
    "### SHIP 2 Dataset:: # missing columns from ship1 (this ship 1 has all columns included) 'Ship_SpeedLOG'\n",
    "### SHIP 3 Dataset:: # missing columns from ship1 'Ship_SpeedLOG', 'ship_speed_actual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94e441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79454396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self):\n",
    "        self.random_state = 42\n",
    "        self.bucket = 'uploads'\n",
    "        self.directory = 'data'\n",
    "        self.features_scaler = MinMaxScaler()\n",
    "        self.y_scaler = MinMaxScaler()\n",
    "        \n",
    "        self.date_col = ['measurement_time']\n",
    "        \n",
    "        self.numerical_columns = [\n",
    "             'cyl_chargeair_press', 'AE_FO_inlet_flow',\n",
    "             'draught_aft_side', 'AE_FO_inlet_Temp',\n",
    "             'engine_speed','DG_1_power',\n",
    "             'DG_2_power','DG_4_power',\n",
    "             'CAC_CW_HT_pressure', 'CAC_in_Low_Temperature_CW_temp',\n",
    "             'propeller_shaft_output', 'propeller_shaft_rpm',\n",
    "             'propeller_shaft_thrust', 'cyl_chargeair_temp',\n",
    "             'cyl_exh_gas_temp_mean','torque',\n",
    "             'AE_FO_outlet_flow', 'AE_FO_outlet_Temp',\n",
    "             'Eng_in_HTCW_press', 'Eng_in_Jacket_HTCW_temp',\n",
    "             'Eng_out_Jacket_HTCW_temp', 'Eng_Relative_load',\n",
    "             'FO_Rack_position', 'FO_inlet_press',\n",
    "             'fueloil_inlet_temperature', 'ME_FO_inlet_flow',\n",
    "             'ME_FO_outlet_Temp', 'ME_FO_outlet_flow',\n",
    "             'LO_Filter_P', 'LO_filter_in_press',\n",
    "             'LO_in_press', 'LO_in_temp',\n",
    "             'LO_out_temp_TC', 'LO_cooler_CW_out_temp'\n",
    "        ]\n",
    "        self.categorical_columns = [\n",
    "            'DG_1_condition','DG_2_condition',\n",
    "            'DG_3_condition','DG_4_condition',\n",
    "            'ship_inclination'\n",
    "        ]\n",
    "        self.monitoring_col_name = 'ME_FO_consumption'\n",
    "        self.monitoring_col = [self.monitoring_col_name]\n",
    "        \n",
    "        self.columns_used = self.date_col + self.numerical_columns + self.categorical_columns + self.monitoring_col\n",
    "        self.for_normalization_cols = self.numerical_columns\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_dataset_(self, ship_id):\n",
    "        client = Minio(\n",
    "            MINIO_HOST,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False\n",
    "        )\n",
    "        data_path = f'{self.directory}/{ship_id}.csv'\n",
    "        obj = client.get_object(self.bucket, data_path)\n",
    "        df = pd.read_csv(obj, parse_dates=self.date_col, usecols=self.columns_used)\n",
    "        return df\n",
    "    \n",
    "    def _concat_vessel_data(self):\n",
    "        ship1_data = self.load_dataset_(ship_id='ship_1')\n",
    "        ship1_data['ship'] = 1\n",
    "        ship2_data = self.load_dataset_(ship_id='ship_2')\n",
    "        ship2_data['ship'] = 2\n",
    "        ship3_data = self.load_dataset_(ship_id='ship_3')\n",
    "        ship3_data['ship'] = 3\n",
    "        all_vessel_data = pd.concat([ship1_data, ship2_data, ship3_data], ignore_index=True)\n",
    "        return all_vessel_data\n",
    "    \n",
    "    def _store_scalers(self,\n",
    "                       features_sc_path='all_ann_feature_scaler.pkl',\n",
    "                       y_sc_path='all_ann_y_scaler.pkl'\n",
    "                      ):\n",
    "        pickle.dump(self.features_scaler, open(features_sc_path, 'wb'))\n",
    "        pickle.dump(self.y_scaler, open(y_sc_path, 'wb'))\n",
    "    \n",
    "    \n",
    "    def _preprocess(self, df):\n",
    "        df = df.dropna()\n",
    "        df[self.for_normalization_cols] = self.features_scaler.fit_transform(\n",
    "            df[self.for_normalization_cols]\n",
    "        )\n",
    "        df[self.monitoring_col] = self.y_scaler.fit_transform(\n",
    "            df[self.monitoring_col]\n",
    "        )\n",
    "        # remove zero variance data\n",
    "        df = df.loc[:, (df != df.iloc[0]).any()]\n",
    "        return df\n",
    "    \n",
    "    def _data_preparation(self, normalized_df, test_size=0.20):\n",
    "        abort_cols = self.monitoring_col + self.date_col\n",
    "        Y = normalized_df[self.monitoring_col]\n",
    "        X = normalized_df[[column for column in normalized_df.columns if column not in abort_cols]]\n",
    "        X_train,X_test,Y_train,Y_test=train_test_split(\n",
    "            X,Y,test_size=test_size,random_state=self.random_state\n",
    "        )\n",
    "        self._store_scalers()\n",
    "        return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6c4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNObject(object):\n",
    "    \n",
    "    def _construct(self, shape, activation_function='relu'):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(shape, activation=activation_function))\n",
    "        model.add(Dense(32, activation=activation_function))\n",
    "\n",
    "        model.add(Dense(64, activation=activation_function))\n",
    "        model.add(Dense(128, activation=activation_function))\n",
    "\n",
    "        model.add(Dense(512, activation=activation_function))\n",
    "        model.add(Dropout(0.1))\n",
    "        \n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def _fit_model_(self,\n",
    "                    shape,\n",
    "                    epochs,\n",
    "                    X_train,\n",
    "                    Y_train,\n",
    "                    X_test,\n",
    "                    Y_test,\n",
    "                    batch_size=100,\n",
    "                    patience=2\n",
    "                   ):\n",
    "        model = self._construct(shape=shape)\n",
    "        model.compile(optimizer='adam', loss='MSE')\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, Y_test), callbacks=[early_stop]\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def _gen_metrics(self, true_value, predicted):\n",
    "        mae = mean_absolute_error(true_value, predicted)\n",
    "        mse = mean_squared_error(true_value, predicted)\n",
    "        r2 = r2_score(true_value, predicted)\n",
    "        \n",
    "        log.info(f'ANN MAE: {mae}')\n",
    "        log.info(f'ANN MSE: {mse}')\n",
    "        log.info(f'ANN R2: {r2}')\n",
    "        metrics_dict = {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'r2': r2\n",
    "        }\n",
    "        return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80fabeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNMlFlowHandler(object):\n",
    "    def __init__(self, experiment_name, model, model_name, metrics_dict):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "    def _log_to_mlflow(self,\n",
    "                       scaler_features_path='all_ann_feature_scaler.pkl',\n",
    "                       scaler_y_path='all_ann_y_scaler.pkl'\n",
    "                      ):\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        with mlflow.start_run():\n",
    "            \n",
    "            mlflow.log_artifact(scaler_features_path)\n",
    "            mlflow.log_artifact(scaler_y_path)\n",
    "            \n",
    "            mlflow.keras.log_model(\n",
    "                self.model,\n",
    "                self.model_name,\n",
    "                registered_model_name=self.model_name\n",
    "            )\n",
    "            \n",
    "            for metrics_tuple in self.metrics_dict.items():\n",
    "                mlflow.log_metric(metrics_tuple[0], metrics_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "160fdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded_obj = DataLoader()\n",
    "all_vessel_data = data_loaded_obj._concat_vessel_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ec51b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_loaded_obj._preprocess(df=all_vessel_data)\n",
    "X_train, X_test, Y_train, Y_test = data_loaded_obj._data_preparation(normalized_df=processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2335cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6270/6270 [==============================] - 20s 3ms/step - loss: 5.0839e-04 - val_loss: 2.3602e-04\n",
      "Epoch 2/10\n",
      "6270/6270 [==============================] - 20s 3ms/step - loss: 2.7399e-04 - val_loss: 2.7704e-04\n",
      "Epoch 3/10\n",
      "6270/6270 [==============================] - 21s 3ms/step - loss: 2.4067e-04 - val_loss: 1.8114e-04\n",
      "Epoch 4/10\n",
      "6270/6270 [==============================] - 21s 3ms/step - loss: 2.2337e-04 - val_loss: 1.5808e-04\n",
      "Epoch 5/10\n",
      "6270/6270 [==============================] - 21s 3ms/step - loss: 2.1701e-04 - val_loss: 1.6547e-04\n",
      "Epoch 6/10\n",
      "6270/6270 [==============================] - 21s 3ms/step - loss: 2.0869e-04 - val_loss: 2.3297e-04\n"
     ]
    }
   ],
   "source": [
    "ann_obj = ANNObject()\n",
    "features_len = X_train.shape[1]\n",
    "test_rows_len = X_test.shape[0]\n",
    "\n",
    "ann_model = ann_obj._fit_model_(\n",
    "    shape=features_len,\n",
    "    epochs=10,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    Y_train=Y_train,\n",
    "    Y_test=Y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6fef2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4899/4899 [==============================] - 5s 985us/step\n",
      "2022-11-30 21:07:39,484 - __main__ - INFO - ANN MAE: 0.008019574136159833\n",
      "2022-11-30 21:07:39,485 - __main__ - INFO - ANN MSE: 0.0002329690064462628\n",
      "2022-11-30 21:07:39,486 - __main__ - INFO - ANN R2: 0.9954946309665357\n"
     ]
    }
   ],
   "source": [
    "predicted = ann_model.predict(X_test)\n",
    "predictions_reshaped = predicted.reshape((test_rows_len))\n",
    "metrics_dict = ann_obj._gen_metrics(Y_test, predictions_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ff73a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 21:07:58,443 - absl - WARNING - Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\PKAPSA~1.EPU\\AppData\\Local\\Temp\\tmpsjx9lx1q\\model\\data\\model\\assets\n",
      "2022-11-30 21:07:58,726 - tensorflow - INFO - Assets written to: C:\\Users\\PKAPSA~1.EPU\\AppData\\Local\\Temp\\tmpsjx9lx1q\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ann_regressor_all_vessels' already exists. Creating a new version of this model...\n",
      "2022/11/30 21:08:48 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ann_regressor_all_vessels, version 2\n",
      "Created version '2' of model 'ann_regressor_all_vessels'.\n"
     ]
    }
   ],
   "source": [
    "experiment_name='fuel_consumption_regression_ann_all_vessels'\n",
    "model_name='ann_regressor_all_vessels'\n",
    "mlflow_obj = ANNMlFlowHandler(\n",
    "    experiment_name=experiment_name,\n",
    "    model=ann_model,\n",
    "    model_name=model_name,\n",
    "    metrics_dict=metrics_dict\n",
    ")\n",
    "mlflow_obj._log_to_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6926c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
